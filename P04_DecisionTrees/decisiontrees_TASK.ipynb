{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pa\n",
    "import numpy as np\n",
    "import pdb\n",
    "import sys\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Helper Funcitons (Do not edit) \n",
    "\n",
    "The Tree_node class represents one Node in a Decisiontree\n",
    "Each node holds a left and a right child if it is not a leaf. If it is a leaf it will contain the partition of the \n",
    "original dataset corresponding to the respective leaf.\n",
    "In a fully grown tree every leaf is pure with respect to the goal variable \n",
    "Each node needs to have a Split that describes how the dataset is partitioned at a specific Node. It is a python tuple \n",
    "containing:\n",
    "The variable in which the dataset is split\n",
    "The cutoff value for the split\n",
    "The goal variable for which the split is optimized\n",
    "for example ('safety', 'high', 'rating') means that the dataset will be split into a partition where safety is high\n",
    "and a rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_node:\n",
    "    original_Data = None\n",
    "    \n",
    "    def __init__(self,split=None,right_child=None, left_child=None):\n",
    "        self.split=split\n",
    "        self.right_child = right_child\n",
    "        self.left_child = left_child\n",
    "\n",
    "    # returns the returns the child in which obs belongs\n",
    "    def return_child(self,obs):\n",
    "        column = self.split[0]\n",
    "                        \n",
    "        if self.is_categorical(column):\n",
    "            if obs[column] == self.split[1]:\n",
    "                return self.right_child\n",
    "            else:\n",
    "                return self.left_child\n",
    "            \n",
    "        else:\n",
    "            if obs[column] >= self.split[1]:\n",
    "                return self.right_child\n",
    "            else:\n",
    "                return self.left_child\n",
    "    \n",
    "    # returns an estimate for the goal variable of obs\n",
    "    def classify(self, obs):\n",
    "        \n",
    "        child = self.return_child(obs)\n",
    "        \n",
    "        if child.__class__.__name__ == 'Tree_node':\n",
    "            return child.classify(obs)\n",
    "    \n",
    "        target_col = self.split[2]\n",
    "        if self.is_categorical(target_col):\n",
    "            #print(\"majority vote\")\n",
    "            return child[target_col].value_counts().keys()[0]\n",
    "        else:\n",
    "            #print(\"average\")\n",
    "            return np.average(child[~child[target_col].isnull()][\"age\"])\n",
    " \n",
    "\n",
    "    def is_categorical(self, column):\n",
    "        category=True\n",
    "        if not Tree_node.original_Data[column].dtype.name == \"category\":\n",
    "            category = False\n",
    "        return category\n",
    "    \n",
    "# returns the giny impurity for data with respect to column (i.e. use your goal variable as column here)    \n",
    "def gini_impurity(data,column, weights=None):\n",
    "    try:\n",
    "        counts = uniquecounts(data, column)\n",
    "        probs = counts/data.shape[0]\n",
    "        if len(probs) == 1:\n",
    "            prob_obs = np.ones(data.shape[0])\n",
    "        else:\n",
    "            la1 = lambda x: probs[probs.index == x][0]\n",
    "            prob_obs = np.array(list(map(la1, data[column])))\n",
    "            prob_obs = np.square(prob_obs)\n",
    "\n",
    "        if weights is None:\n",
    "            weights = np.ones(data.shape[0])\n",
    "        weights = weights/sum(weights)\n",
    "        return 1-sum(weights*prob_obs)\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        raise\n",
    "       \n",
    "\n",
    "# Count ocurrences of goal variable\n",
    "def uniquecounts(data, column):\n",
    "   val_cnt = data[column].value_counts()\n",
    "   return val_cnt.drop(val_cnt[val_cnt == 0].index)\n",
    "\n",
    "\n",
    "# This is a helper function, that partitions a dataset with respect to a given variable and a cutoff value\n",
    "def divideset(in_set, column, value):\n",
    "   # Make a function that tells us if a row is in\n",
    "   # the first group (true) or the second group (false)\n",
    "   split_function=None\n",
    "   if not in_set[column].dtype.name == \"category\":\n",
    "      # assume it to be numerical if not category\n",
    "      split_function=lambda in_set:in_set[column]>=value\n",
    "   else:\n",
    "      split_function=lambda in_set:in_set[column]==value\n",
    "                                   \n",
    "   # Divide the rows into two sets and return them\n",
    "   set1= in_set[split_function(in_set)].copy()\n",
    "   set2= in_set[np.invert(split_function(in_set))].copy()\n",
    "   return (set1,set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Load Data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_car = pa.read_csv('car.data.csv', sep=\",\")\n",
    "dat_car.dtypes\n",
    "for i in dat_car.columns.values:\n",
    "    dat_car[i] = dat_car[i].astype('category')\n",
    "\n",
    "dat = dat_car\n",
    "target_col = \"rating\"\n",
    "\n",
    "# shuffle data\n",
    "np.random.seed(42)\n",
    "dat = dat.reindex(np.random.permutation(dat.index))\n",
    "\n",
    "# split data into training and test set -- this is absolutely central to fitting a ML model\n",
    "# if you are not shure why, ask your tutor (its going to be on the final exam)\n",
    "split = int(dat.shape[0]/100*20)\n",
    "                         \n",
    "dat_test = dat.iloc[0:split]\n",
    "dat_train = dat.iloc[(split+1):dat.shape[0]]\n",
    "\n",
    "Tree_node.original_Data = dat_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0\n",
    "Look at the dataset. The dataset contains information used cars that are for sale including a rating of the offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1295, 7)\n",
      "['buy_price', 'maintenance', 'doors', 'persons', 'lug_space', 'safety', 'rating']\n",
      "['unacc', 'acc', 'vgood', 'good']\n"
     ]
    }
   ],
   "source": [
    "print(dat_train.shape)\n",
    "\n",
    "print(list(dat_train.columns))\n",
    "\n",
    "print(list(dat_train.rating.unique()))\n",
    "# In this exercise we will fit a model to the remainig six variables in order to predict \"rating\", our goal variable.\n",
    "# You will have to add your own code at each failing assert like this one\n",
    "# assert False, \"Please comment me\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Fit a tree stump: in this task you will fit a tree stump (i.e. a Decision Tree of depth 1) to the used cars data. You will have to find the partition of the input dataset that yields the biggest gini score gain with respect to the goal variable. you will accomplish this by exhaustive search i.e. try every possible partition.\n",
    "\n",
    "# Input Parameters:\n",
    "1. in_set: Training data (features)\n",
    "2. traget_col: Ground truth labels\n",
    "3. weights: Weight of the samples \n",
    "\n",
    "# Outputs:\n",
    "1. best_split: A tuple including the most discriminant (best) feature for splitting, splitting value and labels\n",
    "2. best_sets: A tuple of two sets, which are the outputs of \"divideset\" function. These sets are the result of dividing the training data based on the best split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n"
     ]
    }
   ],
   "source": [
    "def find_best_split(in_set, target_col, weights=None):\n",
    "    # compute the giniy score for the unpartitioned dataset\n",
    "    in_score = gini_impurity(in_set, target_col,weights)\n",
    "\n",
    "    best_gain = 0\n",
    "    best_split = None\n",
    "    best_sets = None\n",
    "    \n",
    "    # try every column\n",
    "    for act_col in in_set.columns.values:\n",
    "        # ignore goal variable - otherwise its trivial\n",
    "        if act_col == target_col: continue\n",
    "        print(act_col)\n",
    "        # construct a list of unique values for this variable \n",
    "        column_values = set(in_set[act_col])\n",
    "        \n",
    "        for col_val in column_values:\n",
    "            (set_1,set_2) = divideset(in_set, act_col, col_val)\n",
    "            p = float(set_1.shape[0])/float(in_set.shape[0])\n",
    "            gain =  in_score-p*gini_impurity(set_1, target_col, weights)-(1-p)*gini_impurity(set_2, target_col, weights) \n",
    "            if(gain > best_gain):\n",
    "                best_gain = gain;\n",
    "                best_split = (act_col, col_val,target_col)\n",
    "                best_sets = (set_1,set_2)\n",
    "\n",
    "        # assert False, \"find_best_split Not implemented yet\"\n",
    "        # Try every possible split of the dataset w.r.t. the current variable\n",
    "        # save the split that yielded the hightest gini gain\n",
    "        # The gini-gain of a partition is defined as follows (assume set is partitioned into part_1 and part_2)\n",
    "        # gain = gini_impurity(set)-p_1*gini_ipurity(part_1)-(1-p_1)*gini_inpurity(part_2)\n",
    "        # where p_1 is nrows(part_1)/nrows(set)\n",
    "        # Hint: see Tree_node comments for the definition of the split\n",
    "\n",
    "  \n",
    "    return best_split, best_sets\n",
    "\n",
    "\n",
    "# Fitting a stump is trivial when you have found the best split\n",
    "split, sets = find_best_split(dat_train, target_col)\n",
    "stump = Tree_node(split,sets[0],sets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Compute the confusion matrix and the correct classification percentage for your tree.\n",
    "\n",
    "# Input Parameters:\n",
    "1. in_data: Data samples (features)   \n",
    "2. target_col: Data labels \n",
    "3. tree: A trained tree for evaluating the samples\n",
    "\n",
    "# Outputs:\n",
    "1. conf_mat: Confusion matrix of the decisions based on the input tree  \n",
    "2. p_correct: Probability of correct decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(in_data, target_col, tree):\n",
    "  \n",
    "    # levels available in the target variable (in this example 4)\n",
    "    levels = uniquecounts(in_data, target_col).keys()\n",
    "    # confusion matrix itself\n",
    "    conf_mat =  np.zeros((len(levels),len(levels)))\n",
    "    # percentage of correct classifications\n",
    "    p_correct = 0\n",
    "    \n",
    "    \n",
    "    count_total = 0\n",
    "    count_correct = 0\n",
    "    for index, row in in_data.iterrows():\n",
    "        prediction = tree.classify(row)\n",
    "        actual = row[target_col]\n",
    "           \n",
    "        conf_mat[pa.Index(levels).get_loc(prediction)][pa.Index(levels).get_loc(actual)] += 1\n",
    "        \n",
    "        count_total += 1\n",
    "        \n",
    "        if (prediction == actual):\n",
    "            count_correct += 1  \n",
    "    \n",
    "    p_correct = count_correct / count_total;\n",
    "    \n",
    "    \n",
    "    return conf_mat, p_correct\n",
    "\n",
    "# Build confusion Matrix with training data\n",
    "conf_mat_stump_train, p_correct_stump_train = conf_matrix(dat_train, target_col,stump)\n",
    "\n",
    "# Build confusion Matrix with test data\n",
    "conf_mat_stump_test, p_correct_stump_test = conf_matrix(dat_test, target_col,stump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Recursively build a tree of variable depth.\n",
    "\n",
    "# Input Parameters:\n",
    "1. in_data: Training data (features)\n",
    "2. traget_col: Ground truth labels\n",
    "3. max_depth: Maximum length of the tree\n",
    "3. weights: Weight of the samples\n",
    "\n",
    "# Output:\n",
    "An instance of the \"Tree_node\" class initialized by a split (output of the \"find_best_split\" function) in addition to the\n",
    "right and left children (recursive call of the \"train_tree\" function for two subset of training data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n",
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n",
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n",
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n",
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n",
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n",
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n",
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n",
      "buy_price\n",
      "maintenance\n",
      "doors\n",
      "persons\n",
      "lug_space\n",
      "safety\n",
      "<__main__.Tree_node object at 0x1168e0828>\n",
      "[[ 806.   44.    3.    0.]\n",
      " [  62.  252.   29.   20.]\n",
      " [   0.    0.    0.    0.]\n",
      " [   0.   22.   26.   31.]]\n",
      "My tree ratio (train): 0.8409266409266409\n",
      "[[ 219.    9.    0.    0.]\n",
      " [  14.   54.    6.    5.]\n",
      " [   0.    3.    8.    6.]\n",
      " [   0.    0.    0.    0.]]\n",
      "My tree ratio (test): 0.8672839506172839\n"
     ]
    }
   ],
   "source": [
    "def train_tree(in_data,target_col,max_depth=99, weigths = None):\n",
    "    # To recursively build a decision tree you have to do two things:\n",
    "    # - if you hit your stopping criterions just return in_data (there are two reasons that stop the recursion)\n",
    "    # - othervise find the best split and call this methon on both set partitions\n",
    "    \n",
    "    # assert False, \"recursively build tree here\"\n",
    "\n",
    "    if max_depth == 0 or len(uniquecounts(in_data, target_col)) == 1:\n",
    "        return in_data\n",
    "    else:\n",
    "        (split,sets) = find_best_split(in_data, target_col, weights = None)\n",
    "        right_child = train_tree(sets[0], target_col, max_depth - 1)\n",
    "        left_child = train_tree(sets[1], target_col, max_depth - 1)\n",
    "        \n",
    "        return Tree_node(split,right_child,left_child)\n",
    "\n",
    "# Tree of depth 5 (fully grown tree is pretty slow, but you can play around with the depth parameter and have a look\n",
    "# at its influence on the classification performance)\n",
    "depth5_tree = train_tree(dat_train, target_col, 5)\n",
    "\n",
    "print(depth5_tree)\n",
    "\n",
    "# Build confusion Matrix with training data\n",
    "conf_mat_5_train, p_correct_5_train = conf_matrix(dat_train, target_col,depth5_tree)\n",
    "\n",
    "# Build confusion Matrix with test data \n",
    "conf_mat_5_test, p_correct_5_test = conf_matrix(dat_test, target_col,depth5_tree)\n",
    "\n",
    "print(conf_mat_5_train)\n",
    "print('My tree ratio (train): {}'.format(p_correct_5_train))\n",
    "\n",
    "print(conf_mat_5_test)\n",
    "print('My tree ratio (test): {}'.format(p_correct_5_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "In this task you repeat task 3 and 2 but use the popular ML library scikit-learn instead of your own with a custom implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8734567901234568"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the variable for use with sklearn - the data has to be encoded this way for sklearn dont worry about the\n",
    "# next 3 lines - just use the _encoded version when you pass data to sklearn\n",
    "d = defaultdict(LabelEncoder)\n",
    "dat_train_encoded = dat_train.apply(lambda x: d[x.name].fit_transform(x))\n",
    "dat_test_encoded = dat_test.apply(lambda x: d[x.name].transform(x))\n",
    "\n",
    "# Hints: have a Look at the DecisionTreeClassifier class. use criterion = \"gini\" and max_depth = 5 to make the results \n",
    "# comparable to task 3. You might have to take a look at the sklearn documentation.\n",
    "# Attention: you pass data to sklearn you have to remove the target variable - otherwise sklearn will use it for \n",
    "# the prediction (e.g. use: dat_train_encoded[dat_train_encoded.columns.difference([target_col])] as training data)\n",
    "# assert False, \"Allocate a decision tree, fit it to the training data and compute the predictions for the goal variable using sklearn\"\n",
    "\n",
    "dat_train_encoded_no_tarvar = dat_train_encoded[dat_train_encoded.columns.difference([target_col])]\n",
    "target_values_class_labels = dat_train_encoded[target_col]\n",
    "dat_test_encoded_without_target_Variable = dat_test_encoded[dat_test_encoded.columns.difference([target_col])]\n",
    "\n",
    "clf_gini = DecisionTreeClassifier(\"gini\", \"best\",max_depth=5)\n",
    "clf_gini.fit(dat_train_encoded_no_tarvar, target_values_class_labels)\n",
    "predictions = clf_gini.predict(dat_test_encoded_without_target_Variable)\n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "# inverse the encoding on the predictions and compute the confusion rate --> how does this compare to your own implementation?\n",
    "predictions = d[target_col].inverse_transform(predictions)\n",
    "\n",
    "# print(predictions)\n",
    "# print(dat_train_encoded_no_tarvar)\n",
    "\n",
    "# print(predictions)\n",
    "sum(dat_test[target_col] == predictions)/float(len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "In this task you will implement the adaboost algorithm to fit a number of trees and use their collective power\n",
    "to build a better classifier. While you can use your own tree implementation i advise you to use DecisionTreeClassifier\n",
    "for performance reasons (on run of adaboost will fit 50 trees). Using the following function you will fits decision trees to the data using adaboost:\n",
    "\n",
    "# Input Parameters (ada_boost_trees):\n",
    "1. in_data: Training data (features)\n",
    "2. column: Ground truth labels \n",
    "3. depth: Depth of the individual trees\n",
    "4. m: Number of trees (hypotheses) to fit\n",
    "\n",
    "# Outputs (ada_boost_trees):\n",
    "1. trees: A list of the fitted trees\n",
    "2. importance = A list of the respective importances (weights) for the fitted trees. These values are used for final weighted decisions \n",
    "\n",
    "# Input Parameters (predict_boosted_trees):\n",
    "1. trees: A list of the fitted trees\n",
    "2. importance = A list of the respective importances (weights) for the fitted trees. These values are used for final weighted decisions \n",
    "3. obs: Data samples for evaluation\n",
    "\n",
    "# Output (predict_boosted_trees):\n",
    "1. trees: A list of the fitted trees\n",
    "2. importance = A list of the respective importances (weights) for the fitted trees. These values are used for final weighted decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My adaboost with trees: 0.9598765432098766\n"
     ]
    }
   ],
   "source": [
    "d = defaultdict(LabelEncoder)\n",
    "def fit_transform(x): \n",
    "    return d[x.name].fit_transform(x)\n",
    "\n",
    "def ada_boost_trees(in_data, column, depth, majority):\n",
    "    trees = []\n",
    "    importance = []\n",
    "\n",
    "    N, _ = in_data.shape\n",
    "    # initialize weights\n",
    "    weights = np.ones(in_data.shape[0]) * float(1) / in_data.shape[0]\n",
    "\n",
    "    for k in range(majority):\n",
    "        d_tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=depth)\n",
    "        d_tree = d_tree.fit(in_data[in_data.columns.difference([target_col])], in_data[target_col], sample_weight=weights)\n",
    "        predictions = d_tree.predict(in_data[in_data.columns.difference([target_col])])\n",
    "\n",
    "        weighted_err = weights.dot(predictions != in_data[column])\n",
    "\n",
    "        # if the weighted error is low enough we just stop\n",
    "        if weighted_err < 1e-200:\n",
    "            break\n",
    "\n",
    "        model_imp = np.log(1 - weighted_err) / (weighted_err)\n",
    "        for i in range(N):\n",
    "            if predictions[i] == in_data[column].iloc[i]:\n",
    "                weights[i] = weights[i]*weighted_err/(1-weighted_err)\n",
    "\n",
    "        trees.append(d_tree)\n",
    "        importance.append(model_imp)\n",
    "\n",
    "    return trees, importance\n",
    "\n",
    "\n",
    "dat_train_encoded = dat_train.apply(fit_transform)\n",
    "trees, importance = ada_boost_trees(dat_train_encoded, target_col, 7, 50)\n",
    "\n",
    "\n",
    "def predict_boosted_trees(trees, importance, obs):\n",
    "    N, _ = obs.shape\n",
    "\n",
    "    predictions_dir = dict()\n",
    "\n",
    "    for (tree, model_imp) in zip(trees, importance):\n",
    "        if model_imp == 0: continue\n",
    "        # majority vote\n",
    "        predictions = tree.predict(obs)\n",
    "        levels = set(predictions)\n",
    "\n",
    "        for level in levels:\n",
    "            if level in predictions_dir.keys():\n",
    "                predictions_dir[level] += (predictions == level) * (model_imp)\n",
    "            else:\n",
    "                predictions_dir[level] = (predictions == level) * (model_imp)\n",
    "\n",
    "    pred = np.zeros((N, len(predictions_dir.keys())))\n",
    "\n",
    "    for k in predictions_dir.keys():\n",
    "        pred[:, k] = predictions_dir[k]\n",
    "\n",
    "    return np.argmin(pred, axis=1)\n",
    "\n",
    "\n",
    "dat_test_encoded = dat_test.apply(fit_transform)\n",
    "predictions = predict_boosted_trees(trees, importance, dat_test_encoded[dat_test_encoded.columns.difference([target_col])])\n",
    "predictions = d[target_col].inverse_transform(predictions)\n",
    "result = sum(dat_test[target_col] == predictions) / float(len(predictions))\n",
    "print('My adaboost with trees: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "Compare our Adaboosted Trees versus sklearn. Fit an sklearn AdaBoostClassifier using a DecisionTreeClassifier as base classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sk-Learn Adaboost with trees: 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# print(\"edit\")\n",
    "# predictions = None\n",
    "# predictions = d[target_col].inverse_transform(predictions)\n",
    "#\n",
    "# sum(dat_test[target_col] == predictions) / float(len(predictions))\n",
    "\n",
    "y_values = dat_train[target_col].copy()\n",
    "y_test_values = dat_test[target_col].copy()\n",
    "\n",
    "del dat_train[target_col]\n",
    "del dat_test[target_col]\n",
    "\n",
    "dat_train_encoded = dat_train.apply(fit_transform)\n",
    "dat_test_encoded = dat_test.apply(fit_transform)\n",
    "\n",
    "d_tree = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5))\n",
    "d_tree.fit(dat_train_encoded, y_values)\n",
    "predictions = d_tree.predict(dat_test_encoded)\n",
    "\n",
    "sum_res = sum(y_test_values == predictions) / float(len(predictions))\n",
    "\n",
    "print('Sk-Learn Adaboost with trees: {}'.format(sum_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
